
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vision Academy Chatbot</title>
    <script src="https://cdn.jsdelivr.net/npm/@langchain/core@1.0.2/dist/index.umd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@langchain/community@0.3.14/dist/index.umd.min.js"></script>
    <style>
        body { font-family: Arial, sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }
        #chatbox { max-width: 600px; margin: auto; background: white; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); padding: 20px; }
        #response { margin-top: 15px; padding: 15px; border: 1px solid #ddd; border-radius: 5px; background: #fafafa; min-height: 50px; }
        input, button { padding: 10px; margin: 5px 0; width: 100%; box-sizing: border-box; }
        button { background: #007bff; color: white; border: none; cursor: pointer; }
        button:hover { background: #0056b3; }
    </style>
</head>
<body>
    <div id="chatbox">
        <h2>Vision Academy AI Assistant</h2>
        <input type="text" id="question" placeholder="Ask a question about Vision Academy..." onkeypress="handleKeyPress(event)">
        <button onclick="getAnswer()">Ask</button>
        <div id="response">Response will appear here...</div>
    </div>

    <script>
        // Global variables for the chain
        let retriever;
        let chain;

        // Initialize the RAG system on window load
        window.onload = async function() {
            document.getElementById('response').innerText = 'Loading AI models... (This may take a moment)';
            
            try {
                // Import necessary LangChain components from the UMD global
                const { HNSWLib } = window.langchainCommunity.vectorstores.HNSWLib;
                const { HuggingFaceTransformersEmbeddings } = window.langchainCommunity.embeddings.HuggingFaceTransformersEmbeddings;
                const { CheerioWebBaseLoader } = window.langchainCommunity.documentLoaders.WebBaseLoader;
                const { RecursiveCharacterTextSplitter } = window.langchainCore.textSplitter;
                const { ContextualCompressionRetriever } = window.langchainCore.retrievers;
                const { LLMChain } = window.langchainCore.chains;
                const { PromptTemplate } = window.langchainCore.prompts;

                // Initialize embeddings
                const embeddings = new HuggingFaceTransformersEmbeddings({
                    model: "Xenova/all-MiniLM-L6-v2"
                });

                // Load and process sample data
                const response = await fetch('sample_data.txt');
                const textData = await response.text();
                
                const splitter = new RecursiveCharacterTextSplitter({
                    chunkSize: 500,
                    chunkOverlap: 50
                });
                
                const docs = await splitter.createDocuments([textData]);
                
                // Create vector store
                const vectorStore = await HNSWLib.fromDocuments(docs, embeddings);
                retriever = vectorStore.asRetriever();

                // Create a simple prompt template
                const template = `Use the following context to answer the question. If you don't know the answer, say you don't know.
                Context: {context}
                Question: {question}
                Answer:`;
                
                const prompt = new PromptTemplate({
                    template: template,
                    inputVariables: ["context", "question"]
                });

                // For true browser LLM, you would integrate WebLLM here
                // For now, we'll use a simple mock LLM for demonstration
                const mockLLM = {
                    async call(prompt) {
                        // This is a mock response - you'll replace this with WebLLM
                        return "This is a mock response. WebLLM integration would go here.";
                    }
                };

                chain = new LLMChain({
                    llm: mockLLM,
                    prompt: prompt
                });

                document.getElementById('response').innerText = 'System ready! Ask a question.';

            } catch (error) {
                console.error("Initialization error:", error);
                document.getElementById('response').innerText = 'Error loading models. Check console.';
            }
        };

        // Handle Enter key press
        function handleKeyPress(event) {
            if (event.key === 'Enter') {
                getAnswer();
            }
        }

        // Main function to get answer
        async function getAnswer() {
            const question = document.getElementById('question').value;
            if (!question) return;
            
            document.getElementById('response').innerText = 'Thinking...';
            
            try {
                // Retrieve relevant documents
                const relevantDocs = await retriever.getRelevantDocuments(question);
                const context = relevantDocs.map(doc => doc.pageContent).join('\n\n');
                
                // Generate answer using the chain
                const result = await chain.call({
                    context: context,
                    question: question
                });
                
                document.getElementById('response').innerText = result.text;
            } catch (error) {
                console.error("Error getting answer:", error);
                document.getElementById('response').innerText = 'Error: ' + error.message;
            }
        }
    </script>
</body>
</html>
